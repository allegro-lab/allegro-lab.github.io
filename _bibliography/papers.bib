@article{operationalizingcontentmoderationaccuracyinthedigitalservicesact,
  title={Operationalizing content moderation "accuracy" in the Digital Services Act},
  author={Johnny Wei, Frederike Zufall, and Robin Jia},
  year={2024},
  journal={aies},
  url={https://arxiv.org/abs/2305.09601},
}
@article{isobenchbenchmarkingmultimodalfoundationmodelsonisomorphicrepresentations,
  title={IsoBench: Benchmarking Multimodal Foundation Models  on Isomorphic Representations},
  author={Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, 
Robin Jia, and Willie Neiswanger},
  year={2024},
  journal={colm},
  url={https://arxiv.org/abs/2404.01266},
  note={dataset: https://huggingface.co/datasets/isobench/IsoBench},
}
@article{provingmembershipinllmpretrainingdataviadatawatermarks,
  title={Proving membership in LLM pretraining data via data watermarks},
  author={Johnny Tian-Zheng Wei, Ryan Yixiang Wang, and Robin Jia},
  year={2024},
  journal={facl},
  url={https://arxiv.org/abs/2402.10892},
}
@article{dolocalizationmethodsactuallylocalizememorizeddatainllms,
  title={Do Localization Methods Actually Localize Memorized Data in LLMs?},
  author={Ting-Yun Chang, Jesse Thomason, and Robin Jia},
  year={2024},
  journal={naacl},
  url={https://arxiv.org/abs/2311.09060},
  aclid={2024.naacl-long.176},
  note={github: https://github.com/terarachang/MemData},
}
@article{efficientendtoendvisualdocumentunderstandingwithrationaledistillation,
  title={Efficient End-to-End Visual Document Understanding with Rationale Distillation},
  author={Wang Zhu, Alekh Agarwal, Mandar Joshi, Robin Jia, Jesse Thomason, 
and Kristina Toutanova},
  year={2024},
  journal={naacl},
  url={https://arxiv.org/abs/2311.09612},
  aclid={2024.naacl-long.465},
}
@article{chainofquestionstrainingwithlatentanswersforrobustmultistepquestionanswering,
  title={Chain-of-Questions Training with Latent Answers for  Robust Multistep Question Answering},
  author={Wang Zhu, Jesse Thomason, and Robin Jia},
  year={2023},
  journal={emnlp},
  url={https://arxiv.org/abs/2305.14901},
  aclid={2023.emnlp-main.547},
  note={github: https://github.com/Bill1235813/QDMR_COQ},
}
@article{sceneselflabeledcounterfactualsforextrapolatingtonegativeexamples,
  title={SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples},
  author={Deqing Fu, Ameya Godbole, and Robin Jia},
  year={2023},
  journal={emnlp},
  url={https://arxiv.org/abs/2305.07984},
  aclid={2023.emnlp-main.485},
  note={github: https://github.com/DeqingFu/scene},
}
@article{estimatinglargelanguagemodelcapabilitieswithoutlabeledtestdata,
  title={Estimating Large Language Model Capabilities without Labeled Test Data},
  author={Harvey Yiyun Fu, Qinyuan Ye, Albert Xu, Xiang Ren, and Robin Jia},
  year={2023},
  journal={femnlp},
  url={https://arxiv.org/abs/2305.14802},
  aclid={2023.findings-emnlp.639},
  note={github: https://github.com/harvey-fin/icl-estimate},
}
@article{howpredictablearelargelanguagemodelcapabilitiesacasestudyonbigbench,
  title={How Predictable Are Large Language Model Capabilities?  A Case Study on BIG-bench},
  author={Qinyuan Ye, Harvey Yiyun Fu, Xiang Ren, and Robin Jia},
  year={2023},
  journal={femnlp},
  url={https://arxiv.org/abs/2305.14947},
  aclid={2023.findings-emnlp.503},
  note={github: https://github.com/INK-USC/predicting-big-bench},
}
@article{datacurationalonecanstabilizeincontextlearning,
  title={Data Curation Alone Can Stabilize In-context Learning},
  author={Ting-Yun Chang and Robin Jia},
  year={2023},
  journal={acl},
  url={https://arxiv.org/abs/2212.10378},
  aclid={2023.acl-long.452},
  note={github: https://github.com/terarachang/DataICL},
}
@article{contrastivenoveltyaugmentedlearninganticipatingoutlierswithlargelanguagemodels,
  title={Contrastive Novelty-Augmented Learning: Anticipating Outliers with  Large Language Models},
  author={Albert Xu, Xiang Ren, and Robin Jia},
  year={2023},
  journal={acl},
  url={https://arxiv.org/abs/2211.15718},
  aclid={2023.acl-long.658},
  note={github: https://github.com/albertkx/CoNAL},
}
@article{aresampleefficientnlpmodelsmorerobust,
  title={Are Sample-Efficient NLP Models More Robust?},
  author={Nelson F. Liu, Ananya Kumar, Percy Liang, and Robin Jia},
  year={2023},
  journal={acl},
  url={https://arxiv.org/abs/2210.06456},
  aclid={2023.acl-short.144},
}
@article{doquestionansweringmodelingimprovementsholdacrossbenchmarks,
  title={Do Question Answering Modeling Improvements Hold Across Benchmarks?},
  author={Nelson F. Liu, Tony Lee, Robin Jia, and Percy Liang},
  year={2023},
  journal={acl},
  url={https://arxiv.org/abs/2102.01065},
  aclid={2023.acl-long.736},
}
@article{doesvlnpretrainingworkwithnonsensicalorirrelevantinstructions,
  title={Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?},
  author={Wang Zhu, Ishika Singh, Yuan Huang, Robin Jia, and Jesse Thomason},
  year={2023},
  journal={odrum},
  url={https://arxiv.org/abs/2311.17280},
}
@article{benchmarkinglongtailgeneralizationwithlikelihoodsplits,
  title={Benchmarking Long-tail Generalization with Likelihood Splits},
  author={Ameya Godbole and Robin Jia},
  year={2023},
  journal={feacl},
  url={https://arxiv.org/abs/2210.06799},
  aclid={2023.findings-eacl.71},
  note={github: https://github.com/ameyagodbole/long-tail-likelihood-splits},
}
@article{generalizationdifferencesbetweenendtoendandneurosymbolicvisionlanguagereasoningsystems,
  title={Generalization Differences between End-to-End and Neuro-Symbolic  Vision-Language Reasoning Systems},
  author={Wang Zhu, Jesse Thomason, and Robin Jia},
  year={2022},
  journal={femnlp},
  url={https://arxiv.org/abs/2210.15037},
  aclid={2022.findings-emnlp.345},
  note={github: https://github.com/Bill1235813/gendiff_vlsys},
}
@article{knowledgebasequestionansweringbycasebasedreasoningoversubgraphs,
  title={Knowledge base question answering by case-based reasoning over subgraphs},
  author={Rajarshi Das, Ameya Godbole, Ankita Naik, Elliot Tower, Manzil Zaheer, 
Hannaneh Hajishirzi, Robin Jia, and Andrew McCallum},
  year={2022},
  journal={icml},
  url={https://proceedings.mlr.press/v162/das22a/das22a.pdf},
  note={github: https://github.com/rajarshd/CBR-SUBG pmlr: https://proceedings.mlr.press/v162/das22a.html},
}
@article{ontherobustnessofreadingcomprehensionmodelstoentityrenaming,
  title={On the Robustness of Reading Comprehension Models to Entity Renaming},
  author={Jun Yan, Yang Xiao, Sagnik Mukherjee, Bill Yuchen Lin, Robin Jia, 
and Xiang Ren},
  year={2022},
  journal={naacl},
  url={https://arxiv.org/abs/2110.08555},
  aclid={2022.naacl-main.37},
  note={github: https://github.com/INK-USC/entity-robustness},
}
@article{modelsintheloopaidingcrowdworkerswithgenerativeannotationassistants,
  title={Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants},
  author={Max Bartolo, Tristan Thrush, Sebastian Riedel, Pontus Stenetorp, Robin Jia, 
and Douwe Kiela},
  year={2022},
  journal={naacl},
  url={https://arxiv.org/abs/2112.09062},
  aclid={2022.naacl-main.275},
  note={github: https://github.com/maxbartolo/synQA-question-generators},
}
@article{questionansweringinfusedpretrainingofgeneralpurposecontextualizedrepresentations,
  title={Question Answering Infused Pre-training of  General-Purpose Contextualized Representations},
  author={Robin Jia, Mike Lewis, and Luke Zettlemoyer},
  year={2022},
  journal={facl},
  url={https://arxiv.org/abs/2106.08190},
  aclid={2022.findings-acl.59},
  note={github: https://github.com/facebookresearch/quip},
}
@article{analyzingdynamicadversarialtrainingdatainthelimit,
  title={Analyzing Dynamic Adversarial Training Data in the Limit},
  author={Eric Wallace, Adina Williams, Robin Jia, and Douwe Kiela},
  year={2022},
  journal={facl},
  url={https://arxiv.org/abs/2110.08514},
  aclid={2022.findings-acl.18},
  note={github: https://github.com/facebookresearch/dadc-limit},
}
@article{oncontinualmodelrefinementinoutofdistributiondatastreams,
  title={On Continual Model Refinement in Out-of-Distribution Data Streams},
  author={Bill Yuchen Lin, Sida Wang, Xi Victoria Lin, Robin Jia, Lin Xiao, Xiang Ren, 
and Scott Yih},
  year={2022},
  journal={acl},
  url={https://arxiv.org/abs/2205.02014},
  aclid={2022.acl-long.223},
  note={website: https://cmr-nlp.github.io/ github: https://github.com/facebookresearch/cmr},
}
@article{dynaboardanevaluationasaserviceplatformforholisticnextgenerationbenchmarking,
  title={Dynaboard: An Evaluation-As-A-Service Platform for  Holistic Next-Generation Benchmarking},
  author={Zhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya Jain, Ledell Wu, 
Robin Jia, Christopher Potts, Adina Williams, and Douwe Kiela},
  year={2021},
  journal={neurips},
  url={https://arxiv.org/abs/2106.06052},
  note={website: https://dynabench.org/ github: https://github.com/facebookresearch/dynalab blog post: https://ai.facebook.com/blog/dynaboard-moving-beyond-accuracy-to-holistic-model-evaluation-in-nlp/},
}
@article{maskedlanguagemodelingandthedistributionalhypothesisorderwordmatterspretrainingforlittle,
  title={Masked Language Modeling and the Distributional Hypothesis:  Order Word Matters Pre-training for Little},
  author={Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, 
Adina Williams, and Douwe Kiela},
  year={2021},
  journal={emnlp},
  url={https://arxiv.org/abs/2104.06644},
  aclid={2021.emnlp-main.230},
  note={github: https://github.com/pytorch/fairseq/tree/main/examples/shuffled_word_order},
}
@article{improvingquestionansweringmodelrobustnesswithsyntheticadversarialdatageneration,
  title={Improving Question Answering Model Robustness with  Synthetic Adversarial Data Generation},
  author={Max Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, 
Pontus Stenetorp, and Douwe Kiela},
  year={2021},
  journal={emnlp},
  url={https://arxiv.org/abs/2104.08678},
  aclid={2021.emnlp-main.696},
  note={model: https://huggingface.co/mbartolo/electra-large-synqa github: https://github.com/maxbartolo/improving-qa-model-robustness task: https://dynabench.org/tasks/qa},
}
@article{towhatextentdohumanexplanationsofmodelbehavioralignwithactualmodelbehavior,
  title={To What Extent do Human Explanations of Model Behavior Align with  Actual Model Behavior?},
  author={Grusha Prasad, Yixin Nie, Mohit Bansal, Robin Jia, Douwe Kiela, 
and Adina Williams},
  year={2021},
  journal={blackbox},
  url={https://arxiv.org/abs/2012.13354},
  aclid={2021.blackboxnlp-1.1},
}
@article{thestatisticaladvantageofautomaticnlgmetricsatthesystemlevel,
  title={The statistical advantage of automatic NLG metrics at the system level},
  author={Johnny Tian-Zheng Wei and Robin Jia},
  year={2021},
  journal={acl},
  url={https://arxiv.org/abs/2105.12437},
  aclid={2021.acl-long.533},
  note={github: https://github.com/johntzwei/metric-statistical-advantage},
}
@article{evaluationexamplesarenotequallyinformativehowshouldthatchangenlpleaderboards,
  title={Evaluation Examples Are Not Equally Informative:  How Should That Change NLP Leaderboards?},
  author={Pedro Rodriguez, Joe Barrow, Alexander Hoyle, John P. Lalor, 
Robin Jia, and Jordan Boyd-Graber},
  year={2021},
  journal={acl},
  url={http://users.umiacs.umd.edu/~jbg//docs/2021_acl_leaderboard.pdf},
  aclid={2021.acl-long.346},
  note={website: https://www.pedro.ai/leaderboard-acl2021 github: https://github.com/facebookresearch/irt-leaderboard},
}
@article{doexplanationshelpusersdetecterrorsinopendomainqaanevaluationofspokenvsvisualexplanations,
  title={Do Explanations Help Users Detect Errors in Open-Domain QA?  An Evaluation of Spoken vs. Visual Explanations},
  author={Ana Valeria Gonzalez, Gagan Bansal, Angela Fan, Yashar Mehdad,
 Robin Jia, and Srinivasan Iyer},
  year={2021},
  journal={facl},
  url={https://aclanthology.org/2021.findings-acl.95.pdf},
  aclid={2021.findings-acl.95},
}
@article{swordsabenchmarkforlexicalsubstitutionwithimproveddatacoverageandquality,
  title={Swords: A Benchmark for Lexical Substitution with  Improved Data Coverage and Quality},
  author={Mina Lee, Chris Donahue, Robin Jia, Alexander Iyabor, and Percy Liang},
  year={2021},
  journal={naacl},
  url={https://arxiv.org/abs/2106.04102},
  aclid={2021.naacl-main.345},
  note={github: https://github.com/p-lambda/swords codalab: https://worksheets.codalab.org/worksheets/0xc924392d555f4b4fbee47be92e3daa0b},
}
@article{dynabenchrethinkingbenchmarkinginnlp,
  title={Dynabench: Rethinking Benchmarking in NLP},
  author={Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, 
Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams},
  year={2021},
  journal={naacl},
  url={https://arxiv.org/abs/2104.14337},
  aclid={2021.naacl-main.324},
  note={website: https://dynabench.org/ github: https://github.com/mlcommons/dynabench},
}
@article{naryrelationpredictionovertextspans,
  title={N-ary relation prediction over text spans},
  author={Hoifung Poon, Cliff Wong, and Robin Jia},
  year={2021},
  journal={us_patent},
  url={https://patents.google.com/patent/US10943068B2/en},
}
@article{ontheimportanceofadaptivedatacollectionforextremelyimbalancedpairwisetasks,
  title={On the Importance of Adaptive Data Collection for  Extremely Imbalanced Pairwise Tasks},
  author={Stephen Mussmann, Robin Jia, and Percy Liang},
  year={2020},
  journal={femnlp},
  url={https://arxiv.org/abs/2010.05103},
  aclid={2020.findings-emnlp.305},
  note={codalab: https://worksheets.codalab.org/worksheets/0x39ba5559790b4099a7ff75f916ce19a4 github: https://github.com/robinjia/adaptive-pairwise},
}
@article{withlittlepowercomesgreatresponsibility,
  title={With Little Power Comes Great Responsibility},
  author={Dallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, 
Kyle Mahowald, and Dan Jurafsky},
  year={2020},
  journal={emnlp},
  url={https://arxiv.org/abs/2010.06595},
  aclid={2020.emnlp-main.745},
  note={github: https://github.com/dallascard/NLP-power-analysis},
}
@article{buildingrobustnaturallanguageprocessingsystems,
  title={Building Robust Natural Language Processing Systems},
  author={Robin Jia},
  year={2020},
  journal={dissertation},
  url={assets/pdf/robinjia_thesis.pdf},
}
@article{selectivequestionansweringunderdomainshift,
  title={Selective Question Answering under Domain Shift},
  author={Amita Kamath, Robin Jia, and Percy Liang},
  year={2020},
  journal={acl},
  url={https://arxiv.org/abs/2006.09462},
  aclid={2020.acl-main.503},
  note={codalab: https://worksheets.codalab.org/worksheets/0xea5a522788f743acb4fbf9e60065be8f},
}
@article{robustencodingsaframeworkforcombatingadversarialtypos,
  title={Robust Encodings: A Framework for Combating Adversarial Typos},
  author={Erik Jones, Robin Jia, Aditi Raghunathan, and Percy Liang},
  year={2020},
  journal={acl},
  url={https://arxiv.org/abs/2005.01229},
  aclid={2020.acl-main.245},
  note={codalab: https://worksheets.codalab.org/worksheets/0x8fc01c7fc2b742fdb29c05669f0ad7d2 github: https://github.com/ejones313/roben},
}
@article{certifiedrobustnesstoadversarialwordsubstitutions,
  title={Certified Robustness to Adversarial Word Substitutions},
  author={Robin Jia, Aditi Raghunathan, Kerem G&#246;ksel, Percy Liang},
  year={2019},
  journal={emnlp},
  url={https://arxiv.org/abs/1909.00986},
  aclid={D19-1423},
  note={codalab: https://worksheets.codalab.org/worksheets/0x79feda5f1998497db75422eca8fcd689 github: https://github.com/robinjia/certified-word-sub},
}
@article{mrqa2019sharedtaskevaluatinggeneralizationinreadingcomprehension,
  title={MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension},
  author={Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen},
  year={2019},
  journal={mrqa},
  url={https://arxiv.org/abs/1910.09753},
  aclid={D19-5801},
  note={github: https://github.com/mrqa/MRQA-Shared-Task-2019},
}
@article{documentlevelnaryrelationextractionwithmultiscalerepresentationlearning,
  title={Document-Level N-ary Relation Extraction with  Multiscale Representation Learning},
  author={Robin Jia, Cliff Wong, and Hoifung Poon},
  year={2019},
  journal={naacl},
  url={https://arxiv.org/abs/1904.02347},
  aclid={N19-1370},
  note={code and data: https://hanover.azurewebsites.net/downloads/naacl2019.aspx},
}
@article{knowwhatyoudontknowunanswerablequestionsforsquad,
  title={Know What You Don't Know: Unanswerable Questions for SQuAD},
  author={Pranav Rajpurkar, Robin Jia, and Percy Liang},
  year={2018},
  journal={acl},
  url={https://arxiv.org/abs/1806.03822},
  aclid={P18-2124},
  note={website: http://stanford-qa.com codalab: https://worksheets.codalab.org/worksheets/0x9a15a170809f4e2cb7940e1f256dee55/ pptx slides: assets/pptx/slides-acl2018-squad2.pptx pdf slides: assets/pdf/slides-acl2018-squad2.pdf},
}
@article{deleteretrievegenerateasimpleapproachtosentimentandstyletransfer,
  title={Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer},
  author={Juncen Li, Robin Jia, He He, and Percy Liang},
  year={2018},
  journal={naacl},
  url={https://arxiv.org/abs/1804.06437},
  aclid={N18-1169},
  note={codalab: https://worksheets.codalab.org/worksheets/0xe3eb416773ed4883bb737662b31b4948/ pptx slides: assets/pptx/slides-naacl2018-drg.pptx pdf slides: assets/pdf/slides-naacl2018-drg.pdf},
}
@article{adversarialexamplesforevaluatingreadingcomprehensionsystems,
  title={Adversarial Examples for Evaluating Reading Comprehension Systems},
  author={Robin Jia and Percy Liang},
  year={2017},
  journal={emnlp},
  url={https://arxiv.org/abs/1707.07328},
  aclid={D17-1215},
  note={codalab: https://worksheets.codalab.org/worksheets/0xc86d3ebe69a3427d91f9aaa63f7d1e7d/ pptx slides: assets/pptx/slides-emnlp2017-adversarial.pptx pdf slides: assets/pdf/slides-emnlp2017-adversarial.pdf},
}
@article{learningconceptsthroughconversationsinspokendialoguesystems,
  title={Learning Concepts through Conversations in Spoken Dialogue Systems},
  author={Robin Jia, Larry Heck, Dilek Hakkani-T&#252;r, and Georgi Nikolov},
  year={2017},
  journal={icassp},
  url={assets/pdf/icassp2017-concepts.pdf},
  note={data: assets/data/concept_learning_data.tar.gz bib: assets/bib/icassp2017-concepts.bib},
}
@article{datarecombinationforneuralsemanticparsing,
  title={Data Recombination for Neural Semantic Parsing},
  author={Robin Jia and Percy Liang},
  year={2016},
  journal={acl},
  url={https://arxiv.org/abs/1606.03622},
  aclid={P16-1002},
  note={codalab: https://worksheets.codalab.org/worksheets/0x50757a37779b485f89012e4ba03b6f4f/ pptx slides: assets/pptx/slides-acl2016-recombination.pptx pdf slides: assets/pdf/slides-acl2016-recombination.pdf},
}
@article{reversegenomicspredictsfunctionofhumanconservednoncodingelements,
  title={"Reverse Genomics" Predicts Function of Human Conserved Noncoding Elements},
  author={Amir Marcovitz, Robin Jia, and Gill Bejerano},
  year={2016},
  journal={mbe},
  url={http://www.ncbi.nlm.nih.gov/pubmed/26744417},
}
@article{mx1andmx2keyantiviralproteinsaresurprisinglylostintoothedwhales,
  title={Mx1 and Mx2 Key Antiviral Proteins are Surprisingly Lost in Toothed Whales},
  author={Benjamin A. Braun, Amir Marcovitz, J. Gray Camp, Robin Jia, and Gill Bejerano},
  year={2015},
  journal={pnas},
  url={http://www.ncbi.nlm.nih.gov/pubmed/26080416},
}
@article{whenpartsaregreaterthansumsindividualllmcomponentscanoutperformfullmodels,
  title={When Parts are Greater Than Sums: Individual LLM Components Can Outperform Full Models},
  author={Ting-Yun Chang, Jesse Thomason, and Robin Jia},
  year={2024},
  journal={arxiv},
  url={https://arxiv.org/abs/2406.13131},
}
@article{pretrainedlargelanguagemodelsusefourierfeaturestocomputeaddition,
  title={Pre-trained Large Language Models Use Fourier Features to Compute Addition},
  author={Tianyi Zhou, Deqing Fu, Vatsal Sharan, and Robin Jia},
  year={2024},
  journal={arxiv},
  url={https://arxiv.org/abs/2406.03445},
}
@article{languagemodelscaninferactionsemanticsforclassicalplannersfromenvironmentfeedback,
  title={Language Models can Infer Action Semantics for Classical Planners from Environment Feedback},
  author={Wang Zhu, Ishika Singh, Robin Jia, and Jesse Thomason},
  year={2024},
  journal={arxiv},
  url={https://arxiv.org/abs/2406.02791},
}
@article{transformerslearnhigherorderoptimizationmethodsforincontextlearningastudywithlinearmodels,
  title={Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models},
  author={Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan},
  year={2023},
  journal={arxiv},
  url={https://arxiv.org/abs/2310.17086},
}
